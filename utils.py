import os
import re 
import markdown
import ollama
import lmstudio as lms
from fpdf import FPDF
from ebooklib import epub
from styles_lib import STYLE_PRESETS
from mistralai import Mistral
from groq import Groq
from cerebras.cloud.sdk import Cerebras
from openai import OpenAI
from dotenv import load_dotenv
import requests
import json

load_dotenv()

# Initialize clients
mistral_client = Mistral(api_key=os.getenv("MISTRAL_API_KEY", ""))
groq_client = Groq(api_key=os.getenv("GROQ_API_KEY", ""))

OLLAMA_BASE_URL = os.getenv("OLLAMA_BASE_URL", "http://localhost:11434")


def apply_style(style_names, system_prompt=""):
    """Augment system prompt with style instructions"""
    styles = [STYLE_PRESETS[name] for name in style_names if name in STYLE_PRESETS]
    if styles:
        # print("="*20)
        # print(f"styles:  {styles}")
        # print("="*20)
        prompt = f"{system_prompt}\n\nSTYLE INSTRUCTIONS:\n{'; '.join(styles)}"
        # print(prompt)
        return prompt
    
    # print(f"Ningun stilo: {system_prompt}")
    return system_prompt


def generate_text(
    prompt: str,
    model: str = "mistral-small-latest",  # Default to Mistral
    temperature: float = 0.7,
    system_prompt: str = None
) -> str:
    """
    Generates text using Mistral API or local Ollama models.
    
    Args:
        prompt (str): Input prompt
        model (str): One of:
            - "mistral-small-latest" | "mistral-medium-latest" (Mistral API)
            - "ollama:mistral" | "ollama:llama2" (Ollama models)
        temperature (float): Creativity control (0-1)
        system_prompt (str): System message to guide the model's behavior
    Returns:
        str: Generated text
    """
    if model.startswith("mistral"):
        print(f"Response generated by: {model}")
        print("================")
        # Mistral API implementation
        response = mistral_client.chat.complete(
            model=model,
            # messages=[ChatMessage(role="user", content=prompt)],
            messages=[
                {"role": "system", "content": system_prompt},
                {"role": "user", "content": prompt}
            ],
            temperature=temperature
        )
        return response.choices[0].message.content
    
    elif model.startswith("groq"):
        print(f"Response generated by groq: {model}")
        print("================")
        m = model.split("/")
        model = m[1]
        if model.startswith("meta-llama"):
            model = "/".join(m[1:])
        # Groq API implementation
        try:
            chat_completion = groq_client.chat.completions.create(
                model=model,
                # messages=messages,
                messages=[
                    {
                        "role": "system",
                        "content": system_prompt,
                    },
                    {
                        "role": "user", 
                        "content": prompt,
                    }
                ],      
                temperature=temperature,
            )
            return chat_completion.choices[0].message.content
        except Exception as e:
            raise Exception(f"Error in chat_completions_create: {e}")
        
    elif model.startswith("cerebras"):
        print(f"Response generated by Cerebras: {model}")
        print("================")
        client = Cerebras(api_key=os.environ.get("CEREBRAS_API_KEY"))
        model = model.split("/")[1]
        
        # cerebras cloud model implementation
        try:
            response = client.chat.completions.create(
                model=model, 
                messages=[
                    {
                        'role': 'system', 
                        'content': system_prompt, 
                    }, 
                    {
                        'role': 'user', 
                        'content': f"{prompt}", 
                    }, 
                ],
                temperature=temperature,
            )
            return response.choices[0].message.content
        
        except requests.exceptions.RequestException as e:
            raise ConnectionError(f"Cerebras connection failed: {str(e)}")
        
        
    elif model.startswith("samba"):
        print(f"Response generated by Sambanova: {model}")
        print("================")
        client = OpenAI(
            base_url="https://api.sambanova.ai/v1/",
            api_key=os.environ.get("SAMBANOVA_API_KEY"),
        )
        model = model.split("/")[1]
        
        # sambanova cloud model implementation
        try:
            completion = client.chat.completions.create(
            model=model,
            messages=[
                {
                    "role": "system",
                    "content": system_prompt},
                {
                    "role": "user", 
                    "content": prompt,
                }
            ],
            temperature=0.7,
            # stream=True,
        )
            return completion.choices[0].message.content
        
        except requests.exceptions.RequestException as e:
            raise ConnectionError(f"Sambanova connection failed: {str(e)}")
    
    elif model.startswith("ollama"):
        print(f"Response generated by ollama: {model}")
        print("================")

        model = model.split("/")[1]
        
        # Ollama local model implementation
        try:
            response = ollama.chat(
                model=model, 
                messages=[
                    {
                        'role': 'system', 
                        'content': system_prompt, 
                    }, 
                    {
                        'role': 'user', 
                        'content': f"{prompt}", 
                    }, 
                ],
                temperature=temperature,
            )
            # print(type(response['message']['content']))
            # print("================")
            return(response['message']['content'])
        except requests.exceptions.RequestException as e:
            raise ConnectionError(f"Ollama connection failed: {str(e)}")
        
    elif model.startswith("LMstudio"):
        
        
        print(f"Response generated by LMstudio: {model}")
        print("================")

        model = model.split("/")[1]
        # LMstudio local model implementation
        try:
            model = lms.llm(model)

            response = model.respond({
                "messages":[
                    {
                        'role': 'system', 
                        'content': system_prompt, 
                    }, 
                    {
                        'role': 'user', 
                        'content': f"{prompt}", 
                    }, 
                ]
            },
            config={"temperature": temperature}                         
            )
            return str(response)
        except requests.exceptions.RequestException as e:
            raise ConnectionError(f"LMstudio connection failed: {str(e)}")
    
    else:
        raise ValueError(f"Unsupported model: {model}")


def refine_text(
    original_text: str,
    user_feedback: str,
    model: str = "mistral-small-latest",
    **kwargs
) -> str:
    """
    Refines text using AI model.
    
    Args:
        original_text (str): Text to refine
        user_feedback (str): Instructions for refinement
        model (str): Which LLM to use
        **kwargs: Additional model-specific args
    
    Returns:
        str: Refined text
    """
    prompt = f"""
    Refine this text based on the feedback below.
    Return ONLY the refined text, no additional commentary.
    
    Feedback: {user_feedback}
    ---
    Text to refine:
    {original_text}
    """
    return generate_text(prompt, model=model, **kwargs)


def split_into_paragraphs(text):
    """Split text into paragraphs while preserving empty lines as paragraph separators"""
    paragraphs = re.split(r'\n\s*\n', text.strip())
    # The if p.strip() condition is necessary if you want to ensure that the resulting list does not contain any empty paragraphs. 
    # If you are okay with having empty paragraphs in the list, you can omit the condition
    return [p.strip() for p in paragraphs if p.strip()]

def join_paragraphs(paragraphs):
    """Join paragraphs with double newlines"""
    return '\n\n'.join(paragraphs)

def delete_paragraph(paragraphs, index):
    """
    Delete a paragraph from a list of paragraphs
    
    Args:
        paragraphs (list): List of paragraph strings
        index (int): Index of paragraph to delete
    
    Returns:
        list: Updated list of paragraphs with specified paragraph removed
    """
    if index < 0 or index >= len(paragraphs):
        raise IndexError("Paragraph index out of range")
    
    return paragraphs[:index] + paragraphs[index+1:]


def regenerate_paragraph(
    paragraph,
    all_paragraphs,  # Pass the full paragraphs list
    paragraph_index,  # Current paragraph index
    instruction=None,
    context_window=2,  # How many paragraphs to consider before/after
    **kwargs
):
    """
    Enhanced regeneration with context awareness
    
    Args:
        paragraph: The paragraph to regenerate
        all_paragraphs: Complete list of all paragraphs
        paragraph_index: Position of current paragraph
        instruction: How to modify the paragraph
        context_window: How many surrounding paragraphs to consider
        **kwargs: Other args for generate_text()
    """
    # Get contextual paragraphs
    prev_paras = all_paragraphs[max(0, paragraph_index-context_window):paragraph_index]
    next_paras = all_paragraphs[paragraph_index+1:paragraph_index+1+context_window]
    
    # Build context-aware prompt
    prompt = f"""Improve this paragraph while maintaining context:
    
    Instruction: {instruction or "Make this more engaging"}

    --- SURROUNDING CONTEXT ---
    {'\n\n'.join(prev_paras) if prev_paras else '[BEGINNING OF DOCUMENT]'}

    <<PARAGRAPH TO IMPROVE>>
    {paragraph}

    {'\n\n'.join(next_paras) if next_paras else '[END OF DOCUMENT]'}
    --- END CONTEXT ---

    Please rewrite the marked paragraph to better fit its context:
    
    Return ONLY the refined text, no additional commentary."""
    
    return generate_text(prompt, **kwargs)


# def regenerate_paragraph(
#     paragraph, 
#     instruction=None, 
#     context = None,
#     context_window=1,  # Number of paragraphs to include before/after
#     model=None, 
#     system_prompt=None, 
#     temperature=0.7
# ):
#     """
#     Regenerate a single paragraph with nearby context.
    
#     Args:
#         paragraph (str): The paragraph to regenerate
#         instruction (str): How to modify the paragraph
#         context (dict): Dictionary containing 'previous_paragraphs' and 'next_paragraphs'
#         context_window (int): How many paragraphs before/after to include
#         model (str): Model identifier
#         system_prompt (str): System message
#         temperature (float): Creativity control
    
#     Returns:
#         str: Regenerated paragraph
#     """
#     if context is None:
#         context = {"previous_paragraphs": [], "next_paragraphs": []}
    
#     if instruction is None:
#         instruction = "Improve this text while maintaining its original meaning and style."
    
#     # messages = []
    
#     # if system_prompt:
#     #     messages.append({"role": "system", "content": system_prompt})
    
#     # Build context-aware prompt
#     prompt = f"""
#     Refine this text based on the instructions below.
#     Return ONLY the refined text, no additional commentary.
    
#     Instructions: {instruction}
#     ---
#     Text to refine:
#     {paragraph}
#     """
#     # prompt = f"""Please refine the following paragraph{' ' + instruction if instruction else ''}:
    
#     # Paragraph to refine:
#     # {paragraph}
#     # """
    
#     if context_window > 0:
#         prev_paras = context.get("previous_paragraphs", [])[-context_window:]
#         next_paras = context.get("next_paragraphs", [])[:context_window]
        
#         if prev_paras:
#             prompt += f"\nPrevious context (for reference only):\n{'\n\n'.join(prev_paras)}\n"
#         if next_paras:
#             prompt += f"\nNext context (for reference only):\n{'\n\n'.join(next_paras)}\n"
    
#     prompt += "\nPlease provide ONLY the regenerated paragraph, without any additional commentary or markup."
    
#     # messages.append({"role": "user", "content": prompt})
    
#     response = generate_text(
#         prompt=prompt,
#         model=model,
#         temperature=temperature,
#         system_prompt=system_prompt
#     )
    
#     return response.strip()


    
from fpdf import FPDF
from io import BytesIO

def generate_pdf(story_content, title="AI Generated Story", author="AI Story Writer"):
    """Generate PDF with Unicode support"""
    pdf = FPDF()
    pdf.set_auto_page_break(auto=True, margin=15)
    
    # Add DejaVu font (or any other Unicode font)
    try:
        pdf.add_font('DejaVu', '', 'DejaVuSans.ttf', uni=True)
        pdf.add_font('DejaVu', 'B', 'DejaVuSans-Bold.ttf', uni=True)
        pdf.add_font('DejaVu', 'I', 'DejaVuSans-Oblique.ttf', uni=True)
        font_family = 'DejaVu'
    except:
        # Fallback to Arial Unicode if DejaVu not available
        try:
            pdf.add_font('ArialUnicode', '', 'arial-unicode-ms.ttf', uni=True)
            font_family = 'ArialUnicode'
        except:
            font_family = 'Arial'
            print("Special characters may not display correctly - install DejaVu or Arial Unicode fonts for full support")
    
    # Add a page
    pdf.add_page()
    
    # Title
    pdf.set_font(font_family, 'B', 16)
    pdf.cell(0, 10, title, 0, 1, 'C')
    
    # Author
    pdf.set_font(font_family, 'I', 12)
    pdf.cell(0, 10, f"by {author}", 0, 1, 'C')
    pdf.ln(10)
    
    # Content
    pdf.set_font(font_family, '', 12)
    for paragraph in story_content.split('\n\n'):
        pdf.multi_cell(0, 8, paragraph.strip())
        pdf.ln(5)
    
    return pdf



from io import BytesIO

def generate_epub(story_content, title="AI Generated Story", author="OMG Writer"):
    """Generate EPUB file from story content and return bytes"""
    # Create EPUB book
    book = epub.EpubBook()
    
    # Create chapter
    chapter = epub.EpubHtml(
        title=title,
        file_name='chapter.xhtml',
        lang='en'
    )
    chapter.content = f"""
    <h1>{title}</h1>
    <h3>by {author}</h3>
    <p>{story_content.replace('\n\n', '</p><p>')}</p>
    """
    
    # Add items to book
    book.add_item(chapter)
    book.add_author(author)
    # book.toc = (epub.Link('chapter.xhtml', 'Content', 'chapter1'),)
    book.add_item(epub.EpubNcx())
    book.add_item(epub.EpubNav())
    book.spine = [ chapter]
    
    # Write to bytes buffer
    buffer = BytesIO()
    epub.write_epub(buffer, book, {})
    buffer.seek(0)
    return buffer.getvalue()
                        
def insert_empty_paragraph(paragraphs, index):
    """Insert an empty paragraph at specified position"""
    return paragraphs[:index+1] + [""] + paragraphs[index+1:]


def move_paragraph_up(paragraphs, index):
    """Move paragraph up in the list"""
    if index <= 0 or index >= len(paragraphs):
        return paragraphs
    paragraphs[index], paragraphs[index-1] = paragraphs[index-1], paragraphs[index]
    return paragraphs

def move_paragraph_down(paragraphs, index):
    """Move paragraph down in the list"""
    if index < 0 or index >= len(paragraphs)-1:
        return paragraphs
    paragraphs[index], paragraphs[index+1] = paragraphs[index+1], paragraphs[index]
    return paragraphs