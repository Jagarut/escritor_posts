import os
import re 
import ollama
from prompt_lib import PROMPT_LIBRARY
from mistralai import Mistral, UserMessage
from dotenv import load_dotenv
import requests
import json

load_dotenv()

# Initialize clients
mistral_client = Mistral(api_key=os.getenv("MISTRAL_API_KEY", ""))
OLLAMA_BASE_URL = os.getenv("OLLAMA_BASE_URL", "http://localhost:11434")


def generate_text(
    prompt: str,
    model: str = "mistral-small-latest",  # Default to Mistral
    temperature: float = 0.7,
    system_prompt: str = None
) -> str:
    """
    Generates text using Mistral API or local Ollama models.
    
    Args:
        prompt (str): Input prompt
        model (str): One of:
            - "mistral-small-latest" | "mistral-medium-latest" (Mistral API)
            - "ollama:mistral" | "ollama:llama2" (Ollama models)
        temperature (float): Creativity control (0-1)
        system_prompt (str): System message to guide the model's behavior
    Returns:
        str: Generated text
    """
    if model.startswith("mistral"):
        print(f"Response generated by: {model}")
        # Mistral API implementation
        response = mistral_client.chat.complete(
            model=model,
            # messages=[ChatMessage(role="user", content=prompt)],
            messages=[
                {"role": "system", "content": system_prompt},
                {"role": "user", "content": prompt}
            ],
            temperature=temperature
        )
        return response.choices[0].message.content
    
    elif model.startswith("ollama"):
        
        
        print(f"Response generated by: {model}")
        print("================")
        # print(f"Prompt: {system_prompt}")
        # print("================")
        model = model.split("/")[1]
        

        # Ollama local model implementation
        try:
            response = ollama.chat(
                model=model, 
                messages=[
                    {
                        'role': 'system', 
                        'content': system_prompt, 
                    }, 
                    {
                        'role': 'user', 
                        'content': f"{prompt}", 
                    }, 
                ]
            )
            # print(type(response['message']['content']))
            # print("================")
            return(response['message']['content'])
        except requests.exceptions.RequestException as e:
            raise ConnectionError(f"Ollama connection failed: {str(e)}")
    
    else:
        raise ValueError(f"Unsupported model: {model}")

def refine_text(
    original_text: str,
    user_feedback: str,
    model: str = "mistral-small-latest",
    **kwargs
) -> str:
    """
    Refines text using Mistral or Ollama.
    
    Args:
        original_text (str): Text to refine
        user_feedback (str): Instructions for refinement
        model (str): Which LLM to use
        **kwargs: Additional model-specific args
    
    Returns:
        str: Refined text
    """
    prompt = f"""
    Refine this text based on the feedback below.
    Return ONLY the refined text, no additional commentary.
    
    Feedback: {user_feedback}
    ---
    Text to refine:
    {original_text}
    """
    return generate_text(prompt, model=model, **kwargs)


def split_into_paragraphs(text):
    """Split text into paragraphs while preserving empty lines as paragraph separators"""
    paragraphs = re.split(r'\n\s*\n', text.strip())
    # The if p.strip() condition is necessary if you want to ensure that the resulting list does not contain any empty paragraphs. 
    # If you are okay with having empty paragraphs in the list, you can omit the condition
    return [p.strip() for p in paragraphs if p.strip()]

def join_paragraphs(paragraphs):
    """Join paragraphs with double newlines"""
    return '\n\n'.join(paragraphs)

def regenerate_paragraph(paragraph, instruction=None, context=None, model=None, system_prompt=None, temperature=0.7):
    """
    Regenerate a single paragraph with optional instructions and context
    
    Args:
        paragraph (str): The paragraph to regenerate
        instruction (str): How to modify the paragraph
        context (dict): Surrounding paragraphs for context
        model (str): Model identifier
        system_prompt (str): System message to guide the model
        temperature (float): Creativity parameter
    
    Returns:
        str: Regenerated paragraph
    """
    messages = []
    
    if system_prompt:
        messages.append({"role": "system", "content": system_prompt})
    
    prompt = f"""Please regenerate the following paragraph{' ' + instruction if instruction else ''}:
    
Paragraph to regenerate:
{paragraph}
"""
    
    if context:
        if context.get("previous_paragraphs"):
            prompt += f"\nPrevious context:\n{'\n'.join(context['previous_paragraphs'])}\n"
        if context.get("next_paragraphs"):
            prompt += f"\nFollowing context:\n{'\n'.join(context['next_paragraphs'])}\n"
    
    prompt += "\nPlease provide only the regenerated paragraph, without any additional commentary or markup."
    
    messages.append({"role": "user", "content": prompt})
    
    # Call your AI model here - this will depend on your specific implementation
    response = generate_text(
        prompt=prompt,
        model=model,
        temperature=temperature,
        system_prompt=system_prompt
    )
    
    # Clean up the response to ensure we only get the paragraph
    return response.strip()

# prompt = "Write a first-kiss scene between two rivals in a candlelit library."

# r = generate_text(prompt,  model="ollama/hermes3:3b")
# # r = generate_text(prompt)
# print(r)
